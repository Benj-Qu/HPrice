{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part II: Predicting Housing Prices - Build Your Own Model (50 pts)\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to train a better model on predicting house price! Please only use models in sklearn. Don't introduce other models by uploading extra libraries to the server.\n",
    "\n",
    "### Grading Scheme\n",
    "\n",
    "Your grade for the project will be based on your test RMSE and your readme.md. The breakdown are as follows:\n",
    "\n",
    "1. Readme.md + Completeness of your ipynb(10 pts)\n",
    "\n",
    "2. \n",
    "Points | 40 | 30 | 25 | 20\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Top 20% | (20%, 40%] | (40%, 70%] | Last 30%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5a0f9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "470dba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7ffe560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Imports You Might Need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Extract Dataset\n",
    "with zipfile.ZipFile('cook_county_contest_data.zip') as item:\n",
    "    item.extractall()\n",
    "\n",
    "### Note: we filtered the data in cook_county_contest_data, \n",
    "####so please use this dataset instead of the old one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b36a2d9",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "This notebook is specifically designed to guide you through the process of exporting your model's predictions on the test dataset for submission so you can see how your model performs.\n",
    "\n",
    "Most of what you have done in project part I should be transferrable here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34366744",
   "metadata": {},
   "source": [
    "## Step 1. Set up all the helper functions for your `create_pipeline` function.\n",
    "\n",
    "Please do that in proj.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e08c66ae",
   "metadata": {},
   "source": [
    "## Step 2. Initiate a pipeline\n",
    "\n",
    "Create a pipeline instance:\n",
    "pipeline = create_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6cb3201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38c523c",
   "metadata": {},
   "source": [
    "## Step 3. Train your model\n",
    "\n",
    "Run the following cell to import the new set of training data to fit your model on. **You can use any regression model, the following is just an example** \n",
    "\n",
    "Your model will predict the **original sale price**, If you take log in the middle, please **transfer back** to the normal vlaues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b1c1cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cook_county_contest_train.csv')\n",
    "# train_data = remove_outliers(train_data, 'Building Square Feet', upper=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "39a76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Sale Price']\n",
    "train_data = train_data.drop(columns=['Sale Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "56bcd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['519370910113-2.gz']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###You can use any model in Sklearn\n",
    "pipeline.fit(train_data, y_train)\n",
    "\n",
    "##Export your pipeline\n",
    "dump(pipeline, '519370910113-2.gz', compress=('gzip', 6))\n",
    "\n",
    "#This saves the pipeline to a compressed file\n",
    "#The compress parameter takes a tuple of the compression method and the compression level, which in this case is ( 'gzip', 6)\n",
    "# The compression level ranges from 0 to 9, with 0 being no compression \n",
    "# and 9 being the highest level of compression. \n",
    "# A higher compression level will result in a smaller file size, but will also take longer to compress and decompress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5adba3b",
   "metadata": {},
   "source": [
    "## Step 4. Cross validation and push your code\n",
    "\n",
    "Do cross-validation on the train set to test the performance of your model. **Push your code to Gitea** and send your model to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f11bce79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217, 66)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8f103d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bc5cde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can do cross-validation here\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# pipeline = create_pipeline()\n",
    "# cv = KFold(n_splits=3, random_state=4710, shuffle=True)\n",
    "# scores = cross_val_score(pipeline, train_data, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "# scores = np.sqrt(np.abs(scores))\n",
    "# scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
