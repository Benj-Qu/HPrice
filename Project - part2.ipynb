{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part II: Predicting Housing Prices - Build Your Own Model (50 pts)\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to train a better model on predicting house price! Please only use models in sklearn. Don't introduce other models by uploading extra libraries to the server.\n",
    "\n",
    "### Grading Scheme\n",
    "\n",
    "Your grade for the project will be based on your test RMSE and your readme.md. The breakdown are as follows:\n",
    "\n",
    "1. Readme.md + Completeness of your ipynb(10 pts)\n",
    "\n",
    "2. \n",
    "Points | 40 | 30 | 25 | 20\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Top 20% | (20%, 40%] | (40%, 70%] | Last 30%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0f9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470dba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffe560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Imports You Might Need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Extract Dataset\n",
    "with zipfile.ZipFile('cook_county_contest_data.zip') as item:\n",
    "    item.extractall()\n",
    "\n",
    "### Note: we filtered the data in cook_county_contest_data, \n",
    "####so please use this dataset instead of the old one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34366744",
   "metadata": {},
   "source": [
    "## Step 1. `create_pipeline` function.\n",
    "\n",
    "See proj.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e08c66ae",
   "metadata": {},
   "source": [
    "## Step 2. Initiate a pipeline\n",
    "\n",
    "Create a pipeline instance:\n",
    "pipeline = create_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb3201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38c523c",
   "metadata": {},
   "source": [
    "## Step 3. Train your model\n",
    "\n",
    "Run the following cell to import the new set of training data to fit your model on. **You can use any regression model, the following is just an example** \n",
    "\n",
    "Your model will predict the **original sale price**, If you take log in the middle, please **transfer back** to the normal vlaues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c1cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cook_county_contest_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd41324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resample the data\n",
    "# from sklearn.utils import resample\n",
    "# train_data_resample = pd.DataFrame()\n",
    "# for i in range(9):\n",
    "#     train_data_inrange = train_data[(train_data['Building Square Feet']>=i*1000) & (train_data['Building Square Feet'] < (i+1)*1000)]\n",
    "#     df_temp = resample(train_data_inrange,\n",
    "#         replace=True,\n",
    "#         n_samples=(i+1)*len(train_data_inrange),\n",
    "#         random_state=4710)\n",
    "#     train_data_resample = pd.concat([train_data_resample, df_temp])\n",
    "# train_data = train_data_resample\n",
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Sale Price']\n",
    "train_data = train_data.drop(columns=['Sale Price'])\n",
    "y_train = y_train / train_data['Land Square Feet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0964b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory\n",
      "steps\n",
      "verbose\n",
      "extract_expense_neighbor\n",
      "extract_expense_town\n",
      "extract_description\n",
      "category_encoding\n",
      "drop_cols\n",
      "preprocessor\n",
      "lin-reg\n",
      "extract_expense_neighbor__accept_sparse\n",
      "extract_expense_neighbor__check_inverse\n",
      "extract_expense_neighbor__feature_names_out\n",
      "extract_expense_neighbor__func\n",
      "extract_expense_neighbor__inv_kw_args\n",
      "extract_expense_neighbor__inverse_func\n",
      "extract_expense_neighbor__kw_args\n",
      "extract_expense_neighbor__validate\n",
      "extract_expense_town__accept_sparse\n",
      "extract_expense_town__check_inverse\n",
      "extract_expense_town__feature_names_out\n",
      "extract_expense_town__func\n",
      "extract_expense_town__inv_kw_args\n",
      "extract_expense_town__inverse_func\n",
      "extract_expense_town__kw_args\n",
      "extract_expense_town__validate\n",
      "extract_description__accept_sparse\n",
      "extract_description__check_inverse\n",
      "extract_description__feature_names_out\n",
      "extract_description__func\n",
      "extract_description__inv_kw_args\n",
      "extract_description__inverse_func\n",
      "extract_description__kw_args\n",
      "extract_description__validate\n",
      "category_encoding__accept_sparse\n",
      "category_encoding__check_inverse\n",
      "category_encoding__feature_names_out\n",
      "category_encoding__func\n",
      "category_encoding__inv_kw_args\n",
      "category_encoding__inverse_func\n",
      "category_encoding__kw_args\n",
      "category_encoding__validate\n",
      "drop_cols__accept_sparse\n",
      "drop_cols__check_inverse\n",
      "drop_cols__feature_names_out\n",
      "drop_cols__func\n",
      "drop_cols__inv_kw_args\n",
      "drop_cols__inverse_func\n",
      "drop_cols__kw_args\n",
      "drop_cols__validate\n",
      "preprocessor__n_jobs\n",
      "preprocessor__remainder\n",
      "preprocessor__sparse_threshold\n",
      "preprocessor__transformer_weights\n",
      "preprocessor__transformers\n",
      "preprocessor__verbose\n",
      "preprocessor__verbose_feature_names_out\n",
      "preprocessor__categorical_cols\n",
      "preprocessor__categorical_cols__categories\n",
      "preprocessor__categorical_cols__drop\n",
      "preprocessor__categorical_cols__dtype\n",
      "preprocessor__categorical_cols__handle_unknown\n",
      "preprocessor__categorical_cols__max_categories\n",
      "preprocessor__categorical_cols__min_frequency\n",
      "preprocessor__categorical_cols__sparse\n",
      "preprocessor__categorical_cols__sparse_output\n",
      "lin-reg__bootstrap\n",
      "lin-reg__ccp_alpha\n",
      "lin-reg__criterion\n",
      "lin-reg__max_depth\n",
      "lin-reg__max_features\n",
      "lin-reg__max_leaf_nodes\n",
      "lin-reg__max_samples\n",
      "lin-reg__min_impurity_decrease\n",
      "lin-reg__min_samples_leaf\n",
      "lin-reg__min_samples_split\n",
      "lin-reg__min_weight_fraction_leaf\n",
      "lin-reg__n_estimators\n",
      "lin-reg__n_jobs\n",
      "lin-reg__oob_score\n",
      "lin-reg__random_state\n",
      "lin-reg__verbose\n",
      "lin-reg__warm_start\n"
     ]
    }
   ],
   "source": [
    "# for parameter in pipeline.get_params():\n",
    "#     print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6c7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Define hyperparameter distributions to search over\n",
    "# param_distributions = {\n",
    "#     'lin-reg__max_depth': [5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__max_features': [1.0, 0.5, 0.2, 0.1, 'sqrt', 'log2'],\n",
    "#     'lin-reg__max_leaf_nodes': [5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__n_estimators': [20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__ccp_alpha': [0.0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n",
    "#     'lin-reg__min_impurity_decrease': [0.0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "#     'lin-reg__random_state': [22, 42, 4710],\n",
    "# }\n",
    "\n",
    "# # Perform randomized search\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     pipeline,\n",
    "#     param_distributions = param_distributions,\n",
    "#     n_iter=10,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1,\n",
    "#     random_state=4710\n",
    "# )\n",
    "\n",
    "# # Fit the model on training data\n",
    "# random_search.fit(train_data, y_train)\n",
    "\n",
    "# # Evaluate the best model on test data\n",
    "# print(\"Best score:\", random_search.best_score_)\n",
    "# print(\"Best params:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bcd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['519370910113-2.gz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###You can use any model in Sklearn\n",
    "pipeline.fit(train_data, y_train)\n",
    "\n",
    "##Export your pipeline\n",
    "dump(pipeline, '519370910113-2.gz', compress=('gzip', 6))\n",
    "\n",
    "#This saves the pipeline to a compressed file\n",
    "#The compress parameter takes a tuple of the compression method and the compression level, which in this case is ( 'gzip', 6)\n",
    "# The compression level ranges from 0 to 9, with 0 being no compression \n",
    "# and 9 being the highest level of compression. \n",
    "# A higher compression level will result in a smaller file size, but will also take longer to compress and decompress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5adba3b",
   "metadata": {},
   "source": [
    "## Step 4. Cross validation and push your code\n",
    "\n",
    "Do cross-validation on the train set to test the performance of your model. **Push your code to Gitea** and send your model to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11bce79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217, 63)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f103d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5cde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can do cross-validation here\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# pipeline = create_pipeline()\n",
    "# cv = KFold(n_splits=3, random_state=4710, shuffle=True)\n",
    "# scores = cross_val_score(pipeline, train_data, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "# scores = np.sqrt(np.abs(scores))\n",
    "# scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
