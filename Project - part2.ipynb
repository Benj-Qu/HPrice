{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part II: Predicting Housing Prices - Build Your Own Model (50 pts)\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to train a better model on predicting house price! Please only use models in sklearn. Don't introduce other models by uploading extra libraries to the server.\n",
    "\n",
    "### Grading Scheme\n",
    "\n",
    "Your grade for the project will be based on your test RMSE and your readme.md. The breakdown are as follows:\n",
    "\n",
    "1. Readme.md + Completeness of your ipynb(10 pts)\n",
    "\n",
    "2. \n",
    "Points | 40 | 30 | 25 | 20\n",
    "--- | --- | --- | --- | ---\n",
    "Test RMSE | Top 20% | (20%, 40%] | (40%, 70%] | Last 30%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0f9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470dba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffe560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Imports You Might Need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Extract Dataset\n",
    "with zipfile.ZipFile('cook_county_contest_data.zip') as item:\n",
    "    item.extractall()\n",
    "\n",
    "### Note: we filtered the data in cook_county_contest_data, \n",
    "####so please use this dataset instead of the old one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34366744",
   "metadata": {},
   "source": [
    "## Step 1. `create_pipeline` function.\n",
    "\n",
    "See proj.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e08c66ae",
   "metadata": {},
   "source": [
    "## Step 2. Initiate a pipeline\n",
    "\n",
    "Create a pipeline instance:\n",
    "pipeline = create_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb3201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38c523c",
   "metadata": {},
   "source": [
    "## Step 3. Train your model\n",
    "\n",
    "Run the following cell to import the new set of training data to fit your model on. **You can use any regression model, the following is just an example** \n",
    "\n",
    "Your model will predict the **original sale price**, If you take log in the middle, please **transfer back** to the normal vlaues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c1cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cook_county_contest_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd41324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Resample the data\n",
    "# from sklearn.utils import resample\n",
    "# train_data_resample = pd.DataFrame()\n",
    "# for i in range(9):\n",
    "#     train_data_inrange = train_data[(train_data['Building Square Feet']>=i*1000) & (train_data['Building Square Feet'] < (i+1)*1000)]\n",
    "#     df_temp = resample(train_data_inrange,\n",
    "#         replace=True,\n",
    "#         n_samples=(i+1)*len(train_data_inrange),\n",
    "#         random_state=4710)\n",
    "#     train_data_resample = pd.concat([train_data_resample, df_temp])\n",
    "# train_data = train_data_resample\n",
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0231d296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2568.0\n",
       "1         1040.0\n",
       "2         1188.0\n",
       "3         2252.0\n",
       "4          787.0\n",
       "           ...  \n",
       "138212     882.0\n",
       "138213    1004.0\n",
       "138214    1085.0\n",
       "138215    1494.0\n",
       "138216     864.0\n",
       "Name: Building Square Feet, Length: 138217, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Building Square Feet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['Sale Price']\n",
    "train_data = train_data.drop(columns=['Sale Price'])\n",
    "y_train = y_train / train_data['Building Square Feet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0964b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parameter in pipeline.get_params():\n",
    "#     print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a6c7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Define hyperparameter distributions to search over\n",
    "# param_distributions = {\n",
    "#     'lin-reg__max_depth': [5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__max_features': [1.0, 0.5, 0.2, 0.1, 'sqrt', 'log2'],\n",
    "#     'lin-reg__max_leaf_nodes': [5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__n_estimators': [20, 50, 100, 200, 500, 1000],\n",
    "#     'lin-reg__ccp_alpha': [0.0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n",
    "#     'lin-reg__min_impurity_decrease': [0.0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1],\n",
    "#     'lin-reg__random_state': [22, 42, 4710],\n",
    "# }\n",
    "\n",
    "# # Perform randomized search\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     pipeline,\n",
    "#     param_distributions = param_distributions,\n",
    "#     n_iter=10,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1,\n",
    "#     random_state=4710\n",
    "# )\n",
    "\n",
    "# # Fit the model on training data\n",
    "# random_search.fit(train_data, y_train)\n",
    "\n",
    "# # Evaluate the best model on test data\n",
    "# print(\"Best score:\", random_search.best_score_)\n",
    "# print(\"Best params:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56bcd784",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline.set_params() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m###You can use any model in Sklearn\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pipeline\u001b[39m.\u001b[39;49mset_params(pipeline\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__max_depth\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m20\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__max_features\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mlog2\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__max_leaf_nodes\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__n_estimators\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m500\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__ccp_alpha\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__min_impurity_decrease\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mlin-reg__random_state\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m42\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m }))\n\u001b[1;32m     11\u001b[0m pipeline\u001b[39m.\u001b[39mfit(train_data, y_train)\n\u001b[1;32m     13\u001b[0m \u001b[39m##Export your pipeline\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline.set_params() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "###You can use any model in Sklearn\n",
    "pipeline.set_params(**{\n",
    "    'lin-reg__max_depth': 20,\n",
    "    'lin-reg__max_features': 'log2',\n",
    "    'lin-reg__max_leaf_nodes': 1000,\n",
    "    'lin-reg__n_estimators': 500,\n",
    "    'lin-reg__ccp_alpha': 0.01,\n",
    "    'lin-reg__min_impurity_decrease': 0.01,\n",
    "    'lin-reg__random_state': 42,\n",
    "})\n",
    "pipeline.fit(train_data, y_train)\n",
    "\n",
    "##Export your pipeline\n",
    "dump(pipeline, '519370910113-2.gz', compress=('gzip', 6))\n",
    "\n",
    "#This saves the pipeline to a compressed file\n",
    "#The compress parameter takes a tuple of the compression method and the compression level, which in this case is ( 'gzip', 6)\n",
    "# The compression level ranges from 0 to 9, with 0 being no compression \n",
    "# and 9 being the highest level of compression. \n",
    "# A higher compression level will result in a smaller file size, but will also take longer to compress and decompress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5adba3b",
   "metadata": {},
   "source": [
    "## Step 4. Cross validation and push your code\n",
    "\n",
    "Do cross-validation on the train set to test the performance of your model. **Push your code to Gitea** and send your model to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bce79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217, 63)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f103d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138217,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5cde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can do cross-validation here\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# pipeline = create_pipeline()\n",
    "# cv = KFold(n_splits=3, random_state=4710, shuffle=True)\n",
    "# scores = cross_val_score(pipeline, train_data, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "# scores = np.sqrt(np.abs(scores))\n",
    "# scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
